{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7418f10",
   "metadata": {},
   "source": [
    "# Standalone generator of an  i.i.d. sequential Gaussian dataset with likelihood ratio.\n",
    "- Will be stored as LMDB format\n",
    "- The number of total generated sequence will be BATCH_SIZE * NUM_ITER * NUM_CLASSES\n",
    "- Modify 'PHASE' parameter as 'train', 'val', 'test' to generate three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf052f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import lmdb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from scipy.stats import multivariate_normal\n",
    "from loguru import logger\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Set, Dict, List\n",
    "\n",
    "\n",
    "######## USER MODIFIABLE BLOCKS ########\n",
    "DENSITY_OFFSET = 0.5\n",
    "BATCH_SIZE = 100\n",
    "NUM_ITER = 10\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "config = {\n",
    "    'FEAT_DIM' : 128, # 2, # dimension of multivariate Gaussian\n",
    "    'DENSITY_OFFSET' : DENSITY_OFFSET, # separation of distributions\n",
    "    'BATCH_SIZE' : BATCH_SIZE, # number of sequence that created at once\n",
    "    'NUM_ITER' : NUM_ITER, # total data number will be batch_size * num_iter * num_classes\n",
    "    'TIME_STEPS' : 50, # 50, # length of time steps\n",
    "    'NUM_CLASSES' : NUM_CLASSES, # class numbers\n",
    "    'PHASE': 'test', # train, val, or test\n",
    "    'IS_SHUFFLE': True,\n",
    "    'SAVE_DIR': f'./tmp/ExampleGaussian_{NUM_CLASSES}class_offset{DENSITY_OFFSET}/'\n",
    "}\n",
    "######## USER MODIFIABLE BLOCKS END ########\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63fd593b",
   "metadata": {},
   "source": [
    "# Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361edb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDBDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    A PyTorch dataset for reading data and labels from an LMDB database.\n",
    "\n",
    "    Attributes:\n",
    "    - lmdb_path (str): The path to the LMDB database.\n",
    "    - env (lmdb.Environment): The LMDB environment object.\n",
    "    - data_size (int): The number of data points in the dataset.\n",
    "\n",
    "    Methods:\n",
    "    - __len__: Returns the number of data points in the dataset.\n",
    "    - __getitem__: Returns the data and label for a given index.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, lmdb_path: str, names: Tuple[str], is_load_onto_memory=True):\n",
    "        '''\n",
    "        Required function for PyTorch Dataset class.\n",
    "        Initializes a new LMDBDataset object.\n",
    "\n",
    "        Args:\n",
    "        -lmdb_path (str): The path to the LMDB database.\n",
    "        -names (Tuple[str]): list of to-be-retrieved data. e.g., ('data', 'label')\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.lmdb_path = lmdb_path\n",
    "        self.names = names\n",
    "        self.is_load_onto_memory = is_load_onto_memory\n",
    "\n",
    "        # Open a read transaction\n",
    "        with lmdb.open(lmdb_path, readonly=True) as env:\n",
    "            with env.begin() as txn:\n",
    "                # Get the total number of data\n",
    "                self.data_size = txn.stat()['entries'] // len(names)\n",
    "                if is_load_onto_memory:\n",
    "                    self.data = []\n",
    "                    for i in range(self.data_size):\n",
    "                        item = {}\n",
    "                        for name in self.names:\n",
    "                            item_bytes = txn.get(f'{i:08}_{name}'.encode('ascii'))\n",
    "                            item[name] = pickle.loads(item_bytes)\n",
    "                        self.data.append(item)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        '''\n",
    "        Required function for PyTorch Dataset class.\n",
    "        Returns the number of data points in the dataset.\n",
    "        '''\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        '''\n",
    "        Required function for PyTorch Dataset class.\n",
    "        Returns the data and label for a given index.\n",
    "\n",
    "        Args:\n",
    "            index: The index of the data point.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the data and label.\n",
    "        '''\n",
    "\n",
    "        if self.is_load_onto_memory:\n",
    "            item = self.data[index]\n",
    "        else:\n",
    "            # Open the LMDB database for each worker\n",
    "            with lmdb.open(self.lmdb_path, readonly=True) as env:\n",
    "                with env.begin(buffers=True) as txn:\n",
    "                    item = {}\n",
    "                    for name in self.names:\n",
    "                        item_bytes = txn.get(f'{index:08}_{name}'.encode('ascii'))\n",
    "                        item[name] = pickle.loads(item_bytes)\n",
    "        # Convert the data and label to PyTorch tensors\n",
    "        _tensors = []\n",
    "        for name in self.names:\n",
    "            if 'label' in name:\n",
    "                _tensors.append(torch.tensor(item[name]).to(torch.int64))\n",
    "            else:\n",
    "                _tensors.append(torch.tensor(item[name]).to(torch.float32))\n",
    "\n",
    "        return tuple(_tensors)\n",
    "\n",
    "def write_lmdb(txn: lmdb.Transaction, data: Tuple[np.ndarray],\n",
    "               names: Tuple[str], pointer: int) -> None:\n",
    "    '''\n",
    "    Writes the data and labels to an LMDB database.\n",
    "\n",
    "    Args:\n",
    "     -txn (lmdb.Environment): An opened LMDB environment object.\n",
    "    - data (tuple of numpy.ndarray): A tuple of numpy arrays containing the data to be saved.\n",
    "    - name (tuple of str): A tuple containing the names of the arrays to be saved.\n",
    "    - offset (int): An integer specifying offset when iterating over a loop.\n",
    "                    Typically offset is equal to the batch size generated per loop.\n",
    "                    e.g., offset = i * batch_size\n",
    "    Returns:\n",
    "    - None\n",
    "    '''\n",
    "    assert len(data) == len(names), 'Number of data and name list must match.'\n",
    "\n",
    "    # Get the number of data points\n",
    "    data_number = data[0].shape[0]\n",
    "    for data_array in data:\n",
    "        assert data_array.shape[0] == data_number,\\\n",
    "            f'Total {data_array.shape[0]=} and {data_number=} does not match!'\n",
    "\n",
    "    # Open a write transaction\n",
    "    for i in tqdm(range(data_number)):\n",
    "        # Write each data array to the database\n",
    "        for j, data_array in enumerate(data):\n",
    "            data_bytes = pickle.dumps(data_array[i])\n",
    "            name = names[j]\n",
    "            txn.put('{:08}_{}'.format(i + pointer, name).encode('ascii'), data_bytes)\n",
    "\n",
    "\n",
    "class ConfigSubset:\n",
    "    '''\n",
    "    A class to store the required key-value pairs as instance variables.\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key.lower(), value)\n",
    "\n",
    "\n",
    "def extract_params_from_config(requirements: Set[str], config: Dict[str, any]) -> ConfigSubset:\n",
    "    '''\n",
    "    Extract necessary (hyper)parameters for each function. The extracted\n",
    "    parameters are stored as a class instance variables for easy access.\n",
    "    Keys of the necessary parameters are defined in the set \"requirements.\"\n",
    "    The keys are converted to lowercase and used for instance variable names.\n",
    "\n",
    "    Use this class to avoid accidentally overwrite the original config function -\n",
    "    remember that python function gets a pointer to the original config dict,\n",
    "    not a copy of it (i.e., config can be modified within a function).\n",
    "\n",
    "    Args:\n",
    "    - requirements (set): required keys for a given function.\n",
    "    - config (dict): the original dictionary containing all necessary parameters.\n",
    "\n",
    "    Returns:\n",
    "    - sub_conf (ConfigSubset): required variables stored as instance variables of a class.\n",
    "    '''\n",
    "\n",
    "    # assert that all the required keys exist in the config file.\n",
    "    missing_keys = requirements.difference(config.keys())\n",
    "    assert not missing_keys, f\"Missing necessary parameters: {', '.join(missing_keys)}\"\n",
    "\n",
    "    sub_conf = ConfigSubset(**{key: config[key] for key in requirements})\n",
    "    return sub_conf\n",
    "\n",
    "\n",
    "def initialize_multivariate_gaussian(conf: ConfigSubset) -> Tuple[np.ndarray, np.ndarray, List]:\n",
    "    '''\n",
    "    Initialize a multivariate Gaussian distribution for each class.\n",
    "\n",
    "    Args:\n",
    "    - conf (ConfigSubset): an instance of the ConfigSubset class containing the following keys:\n",
    "        - num_classes (int): the number of classes.\n",
    "        - feat_dim (int): the feature dimension.\n",
    "        - density_offset (float): the density offset used to initialize the mean vectors.\n",
    "\n",
    "    Returns:\n",
    "    - meanvecs (ndarray): an array of shape (num_classes, feat_dim) containing the mean vectors for each class.\n",
    "    - covmat (ndarray): the covariance matrix, which is a diagonal matrix of shape (feat_dim, feat_dim).\n",
    "    - pdfs (list): a list of multivariate normal distributions, one for each class.\n",
    "    '''\n",
    "\n",
    "    meanvecs = np.zeros((conf.num_classes, conf.feat_dim))\n",
    "    covmat = np.eye(conf.feat_dim)\n",
    "    pdfs = []\n",
    "    for cls_i in range(conf.num_classes):\n",
    "        meanvecs[cls_i, cls_i] = conf.density_offset\n",
    "        pdfs.append(multivariate_normal(meanvecs[cls_i], covmat))\n",
    "    return meanvecs, covmat, pdfs\n",
    "\n",
    "\n",
    "def compute_log_likelihood_ratio_matrix(x: np.ndarray, pdfs: List, conf: ConfigSubset) -> np.ndarray:\n",
    "    '''\n",
    "    Compute the log-likelihood ratio matrix for each sample in x.\n",
    "\n",
    "    Args:\n",
    "    - x (ndarray): an array of shape (batch_size, feat_dim) containing the feature vectors.\n",
    "    - pdfs (list): a list of multivariate normal distributions, one for each class.\n",
    "    - conf (ConfigSubset): an instance of the ConfigSubset class containing the following keys:\n",
    "        - num_classes (int): the number of classes.\n",
    "        - batch_size (int): the number of samples.\n",
    "        - feat_dim (int): the feature dimension.\n",
    "\n",
    "    Returns:\n",
    "    - llrm (ndarray): an array of shape (batch_size, num_classes, num_classes) containing the log-likelihood ratio\n",
    "      matrix for each sample in x.\n",
    "    '''\n",
    "\n",
    "    likelihood = np.zeros((conf.batch_size, conf.num_classes))\n",
    "    for cls_i in range(conf.num_classes):\n",
    "        likelihood[:, cls_i] = np.log(pdfs[cls_i].pdf(x))\n",
    "\n",
    "    llrm = np.zeros((conf.batch_size, conf.num_classes, conf.num_classes))\n",
    "    for cls_i in range(conf.num_classes):\n",
    "        for cls_j in range(conf.num_classes):\n",
    "            # diagonal is zero by definition\n",
    "            llrm[:, cls_i, cls_j] = likelihood[:, cls_i] - likelihood[:, cls_j]\n",
    "\n",
    "    return llrm\n",
    "\n",
    "\n",
    "def plot_likelihood_ratio_matrix(llrm: np.ndarray, gt_labels: np.ndarray, num_trajectories: int = 50) -> None:\n",
    "    '''\n",
    "    Plot the likelihood ratio matrix for each pair of classes in llrm.\n",
    "\n",
    "    Args:\n",
    "    - llrm (ndarray): an array of shape (batch_size, num_classes, num_classes) containing the log-likelihood ratio\n",
    "      matrix for each sample in the dataset.\n",
    "    - gt_labels (ndarray): an array of shape (batch_size) containing the ground truth labels for the samples in the dataset.\n",
    "    - num_trajectories (int, optional): the number of trajectories to plot for each pair of classes. Defaults to 50.\n",
    "    '''\n",
    "    num_classes = llrm.shape[-1]\n",
    "    colors = plt.cm.tab10(range(num_classes))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "\n",
    "            plt.subplot(num_classes, num_classes, (j + 1) + num_classes * i)\n",
    "            for k, c in zip(range(num_classes), colors):\n",
    "                plt.plot(np.transpose(llrm[gt_labels==k, :, i, j][:num_trajectories]), color=c)\n",
    "\n",
    "    # labels\n",
    "    plt.subplot(num_classes, num_classes, 1)\n",
    "    for k, c, in zip(range(num_classes), colors):\n",
    "        plt.plot(np.transpose(llrm[gt_labels==k, :, 0, 0][:1]), color=c, label=f'class {k}')\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def iid_multivariate_gaussian(meanvecs: np.ndarray, convmat: np.ndarray, pdfs: List, conf: ConfigSubset) \\\n",
    "                                -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    '''\n",
    "    Generate sequential multivariate Gaussian dataset with likelihood ratio of classes.\n",
    "    Args:\n",
    "    - meanvecs (ndarray): an array of shape (num_classes, feat_dim) containing the mean vectors for each class.\n",
    "    - covmat (ndarray): the covariance matrix of shape (feat_dim, feat_dim).\n",
    "    - pdfs (list): a list of multivariate normal distributions, one for each class.\n",
    "    - conf (ConfigSubset): an instance of the ConfigSubset class containing the following keys:\n",
    "        - num_classes (int): the number of classes.\n",
    "        - feat_dim (int): the feature dimension.\n",
    "        - batch_size (int): the number of samples to process at once.\n",
    "        - time_steps (int): the number of time steps (duration) in the generated dataset.\n",
    "\n",
    "    Returns:\n",
    "    - x_iter (ndarray): an array of shape (num_classes * batch_size, time_steps, feat_dim) containing the generated data.\n",
    "    - y_iter (ndarray): an array of shape (num_classes * batch_size) containing the class labels.\n",
    "    - llrm_iter (ndarray): an array of shape (num_classes * batch_size, time_steps, num_classes, num_classes)\n",
    "      containing the likelihood ratio matrix for each sample in the generated data.\n",
    "\n",
    "    '''\n",
    "    x_cls_pool = []\n",
    "    y_cls_pool = []\n",
    "    llrm_cls_pool = []\n",
    "    for cls_i in range(conf.num_classes):\n",
    "        y = cls_i * np.ones((conf.batch_size))\n",
    "\n",
    "        x_time_pool = []\n",
    "        llrm_time_pool = []\n",
    "        for t_i in range(conf.time_steps):\n",
    "\n",
    "            x = np.random.multivariate_normal(meanvecs[cls_i], covmat, conf.batch_size).astype('float32')\n",
    "            llrm = compute_log_likelihood_ratio_matrix(x, pdfs, conf)\n",
    "\n",
    "            x_time_pool.append(x)\n",
    "            llrm_time_pool.append(llrm)\n",
    "\n",
    "        x_cls = np.stack(x_time_pool, axis=1) # reshape into (BATCH_SIZE, TIME_STEPS, FEAT_DIM)\n",
    "        llrm_cls = np.stack(llrm_time_pool, axis=1) # reshape into (BATCH_SIZE, TIME_STEPS, NUM_CLASSES, NUM_CLASSES)\n",
    "        assert x_cls.shape == (conf.batch_size, conf.time_steps, conf.feat_dim)\n",
    "        assert y.shape == (conf.batch_size,) # size y: (BATCH_SIZE)\n",
    "        assert llrm_cls.shape == (conf.batch_size, conf.time_steps, conf.num_classes, conf.num_classes)\n",
    "        x_cls_pool.append(x_cls)\n",
    "        y_cls_pool.append(y)\n",
    "        llrm_cls_pool.append(llrm_cls)\n",
    "\n",
    "    x_iter = np.concatenate(x_cls_pool, axis=0) # reshape into (NUM_CLASSES * BATCH_SIZE, TIME_STEPS, FEAT_DIM)\n",
    "    y_iter = np.concatenate(y_cls_pool, axis=0) # reshape into (NUM_CLASSES * BATCH_SIZE)\n",
    "    llrm_iter = np.concatenate(llrm_cls_pool, axis=0) # reshape into (NUM_CLASSES * BATCH_SIZE, TIME_STEPS, NUM_CLASSES, NUM_CLASSES)\n",
    "    assert x_iter.shape == (conf.num_classes * conf.batch_size, conf.time_steps, conf.feat_dim)\n",
    "    assert y_iter.shape == (conf.num_classes * conf.batch_size,)\n",
    "    assert llrm_iter.shape == (conf.num_classes * conf.batch_size, conf.time_steps, conf.num_classes, conf.num_classes)\n",
    "\n",
    "    # accumulate evidence\n",
    "    llrm_iter = np.cumsum(llrm_iter, axis=1)\n",
    "\n",
    "    if conf.is_shuffle:\n",
    "        logger.info('shuffling the data...')\n",
    "        total_data = conf.num_classes * conf.batch_size\n",
    "        dice = np.random.permutation(total_data)\n",
    "\n",
    "        x_iter = x_iter[dice]\n",
    "        y_iter = y_iter[dice]\n",
    "        llrm_iter = llrm_iter[dice]\n",
    "\n",
    "    # create a data triplet\n",
    "    return (x_iter, y_iter, llrm_iter)\n",
    "\n",
    "\n",
    "class StopWatch:\n",
    "    '''\n",
    "    A context manager for measureing the elapsed time of a code block.\n",
    "\n",
    "    Usage:\n",
    "        with StopWatch() as time:\n",
    "            <code block>\n",
    "        print('elapsed time (sec):', time.elapsed)\n",
    "    '''\n",
    "    def __init__(self, unit='hours', label=''):\n",
    "        '''\n",
    "        Args:\n",
    "        - unit (str): specifies time unit \\in {seconds, minutes, hours}\n",
    "        - label (str): added to log message\n",
    "        '''\n",
    "        if 'sec' in unit:\n",
    "            self.denom = 1.\n",
    "            self.unit = 'seconds'\n",
    "        elif 'min' in unit:\n",
    "            self.denom = 60.\n",
    "            self.unit = 'minutes'\n",
    "        elif 'hour' in unit:\n",
    "            self.denom = 3600.\n",
    "            self.unit = 'hours'\n",
    "        self.label = label\n",
    "\n",
    "    def __enter__(self):\n",
    "\n",
    "        self.start = time.time()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed = time.time() - self.start\n",
    "        logger.info(f'elapsed time: {self.elapsed / self.denom} {self.unit}. ({self.label})')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "245ee988",
   "metadata": {},
   "source": [
    "# Generate and save LMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strings to name data names in lmdb\n",
    "names = ('data', 'label', 'llr')\n",
    "\n",
    "# The maximum size of the LMDB database in bytes. Default is 1 terabyte.\n",
    "map_size = int(1e12)\n",
    "\n",
    "if not os.path.exists(config['SAVE_DIR']):\n",
    "    os.makedirs(config['SAVE_DIR'])\n",
    "\n",
    "config['LMDB_PATH'] = f'{config[\"SAVE_DIR\"]}{config[\"PHASE\"]}_'\\\n",
    "                      f'{config[\"BATCH_SIZE\"] * config[\"NUM_ITER\"] * config[\"NUM_CLASSES\"]}'\n",
    "\n",
    "# check if necessary parameters are defined in the config file\n",
    "requirements = set(['FEAT_DIM', 'DENSITY_OFFSET', 'BATCH_SIZE', 'IS_SHUFFLE',\n",
    "                    'NUM_ITER', 'TIME_STEPS', 'NUM_CLASSES', 'LMDB_PATH'])\n",
    "conf = extract_params_from_config(requirements, config)\n",
    "\n",
    "# Open a new LMDB database\n",
    "with lmdb.open(conf.lmdb_path, map_size=map_size) as env:\n",
    "    with env.begin(write=True) as txn:\n",
    "        meanvecs, covmat, pdfs = initialize_multivariate_gaussian(conf)\n",
    "\n",
    "        for iter_i in range(conf.num_iter):\n",
    "            logger.info(f'Starting {iter_i=} / {conf.num_iter - 1}')\n",
    "            data = iid_multivariate_gaussian(meanvecs, covmat, pdfs, conf)\n",
    "            write_lmdb(txn, data, names, pointer=iter_i * len(data[0]))\n",
    "\n",
    "logger.info(f'total data genarated:{conf.num_classes * conf.batch_size * conf.num_iter}')\n",
    "logger.success(\"done and dusted!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "577b7af9",
   "metadata": {},
   "source": [
    "# Load LMDB: sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need 'names' to load data correctly\n",
    "dataset = LMDBDataset(conf.lmdb_path, names=names)\n",
    "\n",
    "# pin_memory for performance\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1000, pin_memory=True, num_workers=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb51825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load one example batch\n",
    "with StopWatch(unit='sec'):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x, y, llr = data\n",
    "print(i + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27440e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "plot_likelihood_ratio_matrix(llr, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d811e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = x[y==0]\n",
    "x_1 = x[y==1]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot()\n",
    "plt.scatter(x_0[:, :, 0], x_0[:, :, 1], alpha=0.1, label='class 0')\n",
    "plt.scatter(x_1[:, :, 0], x_1[:, :, 1], alpha=0.1, label='class 1')\n",
    "leg = plt.legend()\n",
    "for lh in leg.legendHandles:\n",
    "    lh._set_alpha_for_array(1)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cb659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e96d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c72872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de58f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d6211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
